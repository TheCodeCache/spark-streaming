package com.stream.test.sparksqs

import org.apache.spark.SparkConf
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.State
import org.apache.spark.streaming.StateSpec
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream.toPairDStreamFunctions

/**
 * Similar to updateStatusByKey but the approach is little different..
 * stateDstream.stateSnapshots().print() is equivalent to pairsDStream.updateStateByKey(updateFunc)
 * 
 * For any Stateful transformations we must set the checkpoint directory..
 */
object StatefulNetworkWordCount extends Main {

  /*if (args.length < 2) {
      System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
      System.exit(1)
    }*/

  //StreamingExamples.setStreamingLogLevels()
  val conf = new SparkConf().setAppName(this.getClass.getSimpleName).setMaster("local[*]")

  val ssc = new StreamingContext(conf, Seconds(5))

  //val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
  // Create the context with a 1 second batch size
  //val ssc = new StreamingContext(sparkConf, Seconds(1))
  ssc.checkpoint("D:\\scala\\file\\checks")

  // Initial state RDD for mapWithState operation
  //val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))
  //val initialRDD = ssc.sparkContext.parallelize(List(("", 0)))
  val initialRDD = ssc.sparkContext.parallelize(List(("", 0)))

  // Create a ReceiverInputDStream on target ip:port and count the
  // words in input stream of \n delimited test (eg. generated by 'nc')
  //val lines = ssc.socketTextStream(args(0), args(1).toInt)
  val lines = ssc.socketTextStream("127.0.0.1", 9999)
  val words = lines.flatMap(_.split(" "))
  //val wordDstream = words.map(x => (x, x.length()))
  val wordDstream = words.map(x => (x, 1))

  // Update the cumulative count using mapWithState
  // This will give a DStream made of state (which is the cumulative count of the words)
  /**
   * this has been implemented with return values wrapped with Option.. we can simply remove the Option type and can work with simple Tuple2 also..
   * 
   * this will be invoked per word and won't be invoked until it scans some data in the stream..
   */
  val mappingFunc = (word: String, one: Option[Int], state: State[Int]) => {
    acc+=1
    println(word+":acc-"+acc)
    if (!word.isEmpty()) {
      println("word::" + word + " one::" + one.getOrElse(0) + " state::" + state.getOption.getOrElse(0))
      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)
      val output = (word, sum)
      state.update(sum)
      Some(output)
    } else {
      None
    }
  }
  val acc = ssc.sparkContext.accumulator(0)
  //StateSpec[String,Int,Int,(String, Int)]
  /**
   * To work with all of this i.e the mapWithState(), make sure RDD CheckPointing is enabled.. otherwise it will throw error for that..
   * First of all, mapWithState can be invoked ONLY on paired DStream (wordDstream).. which is defined in the implicit class called PairDStreamFuntions
   * 
   * here:-
   * 
   * String is the word type
   * Int is "one" type which is Option[Int]
   * Int is "state" type which is State[Int]
   * Tuple2[Int, Int] is the return type of mappingFunc
   * 
   * we can change all of the type information except second Int i.e "one" type.. I mean this is what I have got SO FAR all the experiments..
   * 
   * this has Option[Tuple2[String, Int]] , we can use without Option also..there is no major changes..we only need to change the method return type..
   * 
   * [String,Int,Int,Option[Tuple2[String, Int]]] is the generic type for the function..
   */
  val stateDstream = wordDstream.mapWithState(StateSpec.function/*[String,Int,Int,Option[Tuple2[String, Int]]]*/(mappingFunc))//.initialState(initialRDD)
  //stateDstream.print() // this will only print the overall status of the current word in the current window/micro-batch..
  stateDstream.stateSnapshots().print() // this will print the entire state of all the words that have been found so far.. so even though there is no data in the stream for a batch interval it will still go ahead and print the entire state of all the words that are discovered so far..
  ssc.start()
  ssc.awaitTermination()
}
